{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GCP Tables For AITA Data\n",
    "**NOTE:** Do not run this file again\n",
    "\n",
    "This is exclusivley for getting my tables up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(\"..\")\n",
    "\n",
    "# load enviornment variables for praw to work later\n",
    "load_dotenv(dotenv_path=Path(\"./settings.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to check if table exists\n",
    "def gcp_table_exists(client: bigquery.Client, table_id: str):\n",
    "    try:\n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Using Google Bigquery, create datasets for the data that is about to be loaded. The dataset will be called **AITA_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_NAME = \"bonion\"\n",
    "DATASET_NAME = \"AITA_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table id definitions\n",
    "post_table_id = \"{}.{}.post_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "comment_table_id = \"{}.{}.comment_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "reply_table_id = \"{}.{}.reply_table\".format(PROJ_NAME, DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all dataset ID's\n",
    "To prevent overwriting pre-existing datasets, get all dataset id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project bonion:\n",
      "\tAITA_dataset\n"
     ]
    }
   ],
   "source": [
    "# Get all datasets \n",
    "datasets = list(client.list_datasets())  # Make an API request.\n",
    "dataset_ids = [dataset.dataset_id for dataset in datasets]\n",
    "project = client.project\n",
    "\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(project))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all Table ID's\n",
    "To prevent overwriting pre-existing tables, get all table id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables contained in 'AITA_dataset':\n",
      "bonion.AITA_dataset.comment_table\n",
      "bonion.AITA_dataset.post_table\n",
      "bonion.AITA_dataset.reply_table\n"
     ]
    }
   ],
   "source": [
    "tables = list(client.list_tables(DATASET_NAME))  # Make an API request.\n",
    "table_ids = [table.table_id for table in tables]\n",
    "\n",
    "print(\"Tables contained in '{}':\".format(DATASET_NAME))\n",
    "for table in tables:\n",
    "    print(\"{}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "if DATASET_NAME not in dataset_ids:\n",
    "    dataset = bigquery.Dataset(\"{}.{}\".format(PROJ_NAME, DATASET_NAME))\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    # send dataset to API for completion\n",
    "    initial_post_dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Table\n",
    "The initial post for each of the comments will need to be stored to reference its content which dictated the reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_table_schema = [\n",
    "    bigquery.SchemaField(\"reddit_post_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_title\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_self_text\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_table_id): \n",
    "    post_table = bigquery.Table(post_table_id, schema=post_table_schema)\n",
    "    post_table.description = \"\"\"\n",
    "        A table which holds popular posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Table\n",
    "This table holds all of the responses for each of the posts in the post table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_table_schema = [\n",
    "    bigquery.SchemaField(\"comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"parent_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=comment_table_id): \n",
    "    post_reply_table = bigquery.Table(comment_table_id, schema=comment_table_schema)\n",
    "    post_reply_table.description = \"\"\"\n",
    "        A table which holds the most popular replys to saved posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reply Table\n",
    "This table will hold replys to the initial post reply. This will be used for also dictating the performance of a reply. Because there can be infinite replies to any given post or reply, each post reply id will be limited to some number of replies. These replies will likley be based off of the same metrics as the original post for \"quality\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_table_schema = [\n",
    "    bigquery.SchemaField(\"reply_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"parent_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"content\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]   \n",
    "\n",
    "if not gcp_table_exists(client, table_id=reply_table_id): \n",
    "    post_reply_top_children_table = bigquery.Table(reply_table_id, schema=reply_table_schema)\n",
    "    post_reply_top_children_table.description = \"\"\"\n",
    "        This table will hold replys to the initial post reply. \n",
    "        This will be used for also dictating the performance of a reply. \n",
    "        Because there can be infinite replies to any given post or reply, \n",
    "        each post reply id will be limited to some number of replies. \n",
    "        These replies will likley be based off of the same metrics as \n",
    "        the original post for \"quality\". \n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_top_children_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
