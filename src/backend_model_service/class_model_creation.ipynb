{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune BERT for Multi-label Classification\n",
    "**Based on work from:** [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb)\n",
    "\n",
    "Using hugging faces' BERT model, I will be training it to classify a string based on multiples classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_gen_model import TextGenModelManager\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig, PreTrainedTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>nta</th>\n",
       "      <th>yta</th>\n",
       "      <th>esh</th>\n",
       "      <th>nah</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>title: (UPDATE) AITA for telling my step-daugh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title: AITA - I missed my daughter’s award cer...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title: AITA For Barring My Husband From The Be...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title: AITA For Firing An Employee After His P...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>title: AITA For Refusing To Crochet Something ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  nta  yta  esh  nah  info\n",
       "0  title: (UPDATE) AITA for telling my step-daugh...    1    0    0    0     0\n",
       "1  title: AITA - I missed my daughter’s award cer...    0    1    0    0     0\n",
       "2  title: AITA For Barring My Husband From The Be...    1    0    0    0     0\n",
       "3  title: AITA For Firing An Employee After His P...    0    1    0    0     0\n",
       "4  title: AITA For Refusing To Crochet Something ...    1    0    0    0     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to view the head of the data\n",
    "class_df = pd.read_csv(\"./saved_data/hot_encoded_class.csv\")\n",
    "class_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/cstainsby/.cache/huggingface/datasets/csv/default-3272b440c9a2ece7/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 364.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./saved_data/hot_encoded_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'nta', 'yta', 'esh', 'nah', 'info'],\n",
       "        num_rows: 276\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['prompt', 'nta', 'yta', 'esh', 'nah', 'info']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_row \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m example_row\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2760\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2765\u001b[0m )\n\u001b[1;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/formatting/formatting.py:575\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/formatting/formatting.py:515\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 515\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['prompt', 'nta', 'yta', 'esh', 'nah', 'info']\""
     ]
    }
   ],
   "source": [
    "example_row = dataset[\"train\"][0]\n",
    "example_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['prompt', 'nta', 'yta', 'esh', 'nah', 'info']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2760\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2765\u001b[0m )\n\u001b[1;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/formatting/formatting.py:575\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/datasets/formatting/formatting.py:515\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 515\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['prompt', 'nta', 'yta', 'esh', 'nah', 'info']\""
     ]
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nta', 'yta', 'esh', 'nah', 'info']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"prompt\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "You cant pass direct text into BERT, it takes input ids. To get these, we must tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2516,\n",
       " 1024,\n",
       " 1006,\n",
       " 10651,\n",
       " 1007,\n",
       " 9932,\n",
       " 2696,\n",
       " 2005,\n",
       " 4129,\n",
       " 2026,\n",
       " 3357,\n",
       " 1011,\n",
       " 2684,\n",
       " 2000,\n",
       " 1523,\n",
       " 2175,\n",
       " 3198,\n",
       " 2014,\n",
       " 2613,\n",
       " 3611,\n",
       " 1524,\n",
       " 2043,\n",
       " 2016,\n",
       " 2356,\n",
       " 2033,\n",
       " 2000,\n",
       " 3477,\n",
       " 2005,\n",
       " 2014,\n",
       " 4946,\n",
       " 9735,\n",
       " 1029,\n",
       " 4180,\n",
       " 1024,\n",
       " 1031,\n",
       " 2434,\n",
       " 2695,\n",
       " 1033,\n",
       " 1006,\n",
       " 16770,\n",
       " 1024,\n",
       " 1013,\n",
       " 1013,\n",
       " 7479,\n",
       " 1012,\n",
       " 2417,\n",
       " 23194,\n",
       " 1012,\n",
       " 4012,\n",
       " 1013,\n",
       " 1054,\n",
       " 1013,\n",
       " 26445,\n",
       " 10760,\n",
       " 12054,\n",
       " 11484,\n",
       " 1013,\n",
       " 7928,\n",
       " 1013,\n",
       " 20868,\n",
       " 18037,\n",
       " 4143,\n",
       " 1013,\n",
       " 9932,\n",
       " 2696,\n",
       " 1035,\n",
       " 2005,\n",
       " 1035,\n",
       " 4129,\n",
       " 1035,\n",
       " 2026,\n",
       " 1035,\n",
       " 3357,\n",
       " 2850,\n",
       " 18533,\n",
       " 2121,\n",
       " 1035,\n",
       " 2000,\n",
       " 1035,\n",
       " 2175,\n",
       " 1035,\n",
       " 3198,\n",
       " 1035,\n",
       " 2014,\n",
       " 1013,\n",
       " 1029,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 3120,\n",
       " 1027,\n",
       " 3745,\n",
       " 1004,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 5396,\n",
       " 1027,\n",
       " 16380,\n",
       " 1035,\n",
       " 10439,\n",
       " 1004,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 2171,\n",
       " 1027,\n",
       " 16380,\n",
       " 6491,\n",
       " 2546,\n",
       " 1007,\n",
       " 4931,\n",
       " 4364,\n",
       " 1012,\n",
       " 2009,\n",
       " 1521,\n",
       " 1055,\n",
       " 2042,\n",
       " 1037,\n",
       " 2204,\n",
       " 1016,\n",
       " 3134,\n",
       " 2144,\n",
       " 1045,\n",
       " 1521,\n",
       " 2310,\n",
       " 102]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = example_row[\"prompt\"]\n",
    "print(type(text))\n",
    "encoding = tokenizer.encode(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] title : ( update ) aita for telling my step - daughter to “ go ask her real dad ” when she asked me to pay for her plane tickets? content : [ original post ] ( https : / / www. reddit. com / r / amitheasshole / comments / irxyza / aita _ for _ telling _ my _ stepdaughter _ to _ go _ ask _ her /? utm _ source = share & utm _ medium = ios _ app & utm _ name = iossmf ) hey guys. it ’ s been a good 2 weeks since i ’ ve [SEP]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nta', 'yta', 'esh', 'nah', 'info']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"prompt\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cstainsby/.cache/huggingface/datasets/csv/default-3272b440c9a2ece7/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ea9db39876a15c6f.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 276\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "Split the dataset into a training, validation, and testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 248\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% train + valid, 10% test\n",
    "trainvalid_test_dataset = encoded_dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "trainvalid_dataset = trainvalid_test_dataset[\"train\"]\n",
    "train_valid_dataset =  trainvalid_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "   'train': trainvalid_test_dataset[\"train\"],\n",
    "   'test': trainvalid_test_dataset['test'],\n",
    "   'validation': train_valid_dataset['test']})\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2516,\n",
       " 1024,\n",
       " 1006,\n",
       " 10651,\n",
       " 1007,\n",
       " 9932,\n",
       " 2696,\n",
       " 2005,\n",
       " 4129,\n",
       " 2026,\n",
       " 3357,\n",
       " 1011,\n",
       " 2684,\n",
       " 2000,\n",
       " 1523,\n",
       " 2175,\n",
       " 3198,\n",
       " 2014,\n",
       " 2613,\n",
       " 3611,\n",
       " 1524,\n",
       " 2043,\n",
       " 2016,\n",
       " 2356,\n",
       " 2033,\n",
       " 2000,\n",
       " 3477,\n",
       " 2005,\n",
       " 2014,\n",
       " 4946,\n",
       " 9735,\n",
       " 1029,\n",
       " 4180,\n",
       " 1024,\n",
       " 1031,\n",
       " 2434,\n",
       " 2695,\n",
       " 1033,\n",
       " 1006,\n",
       " 16770,\n",
       " 1024,\n",
       " 1013,\n",
       " 1013,\n",
       " 7479,\n",
       " 1012,\n",
       " 2417,\n",
       " 23194,\n",
       " 1012,\n",
       " 4012,\n",
       " 1013,\n",
       " 1054,\n",
       " 1013,\n",
       " 26445,\n",
       " 10760,\n",
       " 12054,\n",
       " 11484,\n",
       " 1013,\n",
       " 7928,\n",
       " 1013,\n",
       " 20868,\n",
       " 18037,\n",
       " 4143,\n",
       " 1013,\n",
       " 9932,\n",
       " 2696,\n",
       " 1035,\n",
       " 2005,\n",
       " 1035,\n",
       " 4129,\n",
       " 1035,\n",
       " 2026,\n",
       " 1035,\n",
       " 3357,\n",
       " 2850,\n",
       " 18533,\n",
       " 2121,\n",
       " 1035,\n",
       " 2000,\n",
       " 1035,\n",
       " 2175,\n",
       " 1035,\n",
       " 3198,\n",
       " 1035,\n",
       " 2014,\n",
       " 1013,\n",
       " 1029,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 3120,\n",
       " 1027,\n",
       " 3745,\n",
       " 1004,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 5396,\n",
       " 1027,\n",
       " 16380,\n",
       " 1035,\n",
       " 10439,\n",
       " 1004,\n",
       " 21183,\n",
       " 2213,\n",
       " 1035,\n",
       " 2171,\n",
       " 1027,\n",
       " 16380,\n",
       " 6491,\n",
       " 2546,\n",
       " 1007,\n",
       " 4931,\n",
       " 4364,\n",
       " 1012,\n",
       " 2009,\n",
       " 1521,\n",
       " 1055,\n",
       " 2042,\n",
       " 1037,\n",
       " 2204,\n",
       " 1016,\n",
       " 3134,\n",
       " 2144,\n",
       " 1045,\n",
       " 1521,\n",
       " 2310,\n",
       " 102]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_test_valid_dataset[\"train\"],\n",
    "    eval_dataset=train_test_valid_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cstainsby/class/dataProj/bonion/src/backend_model_service/model_env/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/155 03:29 < 05:16, 0.29 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227275</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/4 00:01 < 00:00, 1.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=155, training_loss=0.22187984835716987, metrics={'train_runtime': 545.0264, 'train_samples_per_second': 2.275, 'train_steps_per_second': 0.284, 'total_flos': 81566624163840.0, 'train_loss': 0.22187984835716987, 'epoch': 5.0})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2272748500108719,\n",
       " 'eval_f1': 0.92,\n",
       " 'eval_roc_auc': 0.95,\n",
       " 'eval_accuracy': 0.92,\n",
       " 'eval_runtime': 3.4647,\n",
       " 'eval_samples_per_second': 7.216,\n",
       " 'eval_steps_per_second': 1.155,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/4 00:02 < 00:01, 0.94 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(train_test_valid_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_matrix_to_label_list(label_matrix):\n",
    "    id_list = []\n",
    "    for label_pred_row in label_matrix:\n",
    "        index = list(label_pred_row).index(1)\n",
    "        id = id2label[index]\n",
    "        id_list.append(id)\n",
    "    return id_list\n",
    "\n",
    "def count_frequency(lst):\n",
    "    freq_dict = {}\n",
    "    for value in lst:\n",
    "        if value in freq_dict:\n",
    "            freq_dict[value] += 1\n",
    "        else:\n",
    "            freq_dict[value] = 1\n",
    "    return freq_dict\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    merged_dict = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key in merged_dict:\n",
    "            merged_dict[key] = (merged_dict[key], value)\n",
    "        else:\n",
    "            merged_dict[key] = value\n",
    "    return merged_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nta': (26, 26), 'esh': (2, 2)}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = id_matrix_to_label_list(train_test_valid_dataset[\"test\"][\"labels\"])\n",
    "predicted = id_matrix_to_label_list(label_ids)\n",
    "\n",
    "real_freq = count_frequency(real)\n",
    "predicted_freq = count_frequency(predicted)\n",
    "\n",
    "total_freqs = merge_dicts(real_freq, predicted_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in total_freqs.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Length (mm)')\n",
    "ax.set_title('Penguin attributes by species')\n",
    "ax.set_xticks(x + width, labels)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "ax.set_ylim(0, 250)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"AITA for telling my husband the nanny is in charge?\n",
    "\n",
    "I want to preface this by saying that I am aware this is a very privileged issue but I’m trying to get some perspective on my opinion.\n",
    "\n",
    "My husband and I have 3 kids that are 10 months, 3 years and 6 years old. My husband has a high profile job and it means he’s gone often. I work a regular 9-5. We originally used daycare for our oldest but my middle was born right when the pandemic began, so we hired a nanny. She originally worked when I did. But by the time baby came around, I was very overwhelmed doing bath and bedtime on my own, on top of developing postpartum depression. After a breakdown, we spoke with the nanny and she agreed to adjust her hours so she’s helping me with dinner, bath and bed.\n",
    "\n",
    "We’ve gotten close over the past 6 months doing this. In many ways, she’s become like a third parent to the kids. She’s so good with them. We’ve created a routine that works well. I tend to the baby during bath and bed, she handles the older 2. It’s a nice rhythm and my mental health has gotten so much better.\n",
    "\n",
    "My husband isn’t traveling all the time but most nights, he isn’t even home for dinner and bed. He will help me weekends he’s home. But because he’s gone so often, he’s reluctant to be firm with the kids.\n",
    "\n",
    "There are times he’s come home when our nanny is there. He tries to help her with bath and bed, but allows the boys to rough house, lets them break the routine and it seriously throws them off and delays bedtime.\n",
    "\n",
    "My nanny shared with me she feels awkward. Obviously she doesn’t want to undermine her employer but it just makes her job harder. But my husband also doesn’t want her to go home when he arrives as he says he can’t handle it alone.\n",
    "\n",
    "I told him if that’s the case, then he needs to defer to the nanny and follow her lead. She knows our boys best and she has to deal with the aftermath when they don’t listen and give her a hard time.\n",
    "\n",
    "My husband feels that she’s just an employee and he’s the dad. His salary does pay for her. However, I don’t feel this is fair to her.\n",
    "\n",
    "I told him he either follows her lead for bed and bath or he doesn’t help at all. He told me I’m allowing the nanny to take over and replace him. AITA?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nta']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model For Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./saved_models/store/AITAclassmodel/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
