{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Notes\n",
    "This notebook will be based off of [this video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which goes into depth on how to build the exact base model I need for this project. I will be copy pasting a lot of his work and annotating it to help myself understand the process of making the Transformer work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "\n",
    "# pytorch functionality\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# data\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "For this example I am going to be using the IMDB dataset but the work/model should be generalizable to all text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = WikiText2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "For now I'm going to keep my tokenizer very simple. You can use a multitude of techniques for tokenizing your corpus. Here is a [library](https://github.com/openai/tiktoken) worth looking into at some point.\n",
    "\n",
    "We are going to be getting very long sequences but small token spaces. This can be changed with this library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(in_data, tokenizer):\n",
    "  counter = Counter()\n",
    "  for string in in_data:\n",
    "    counter.update(tokenizer(string))\n",
    "\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def data_process(in_data, tokenizer, vocab: Vocab):\n",
    "  raw_iter = iter(in_data)\n",
    "  data = []\n",
    "  for raw in raw_iter:\n",
    "    tensor = torch.tensor([vocab[token] for token in tokenizer(raw)], dtype=torch.long)\n",
    "    data.append(tensor)\n",
    "    \n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Tokenizer\n",
    "\n",
    "I am going to be doing something slightly different to the video. I'm choosing to use the provided torch tokenizer for words rather than doing it char by char. Torch's tools support this kind of work more but it will require some slight adjustments to the work done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vocab Size: 28785\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(train_iter, tokenizer)\n",
    "\n",
    "print(\"Train Vocab Size:\", len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Example for how encoding and decoding works with Vocab Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There' after encoding: 248\n",
      "'There' after decoding there\n"
     ]
    }
   ],
   "source": [
    "encoded_word = vocab.get_stoi()[\"there\"]\n",
    "decoded_word = vocab.get_itos()[encoded_word]\n",
    "\n",
    "print(\"'There' after encoding:\", encoded_word)\n",
    "print(\"'There' after decoding\", decoded_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Tensor Format\n",
    "Using above data_process function, build a torch tensor representation based on the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape and Type: torch.Size([2049990]) torch.int64\n",
      "Validation Data Shape and Type: torch.Size([214417]) torch.int64\n",
      "Testing Data Shape and Type: torch.Size([241859]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_data = data_process(train_iter, tokenizer, vocab)\n",
    "val_data = data_process(val_iter, tokenizer, vocab)\n",
    "test_data = data_process(test_iter, tokenizer, vocab)\n",
    "\n",
    "print(\"Training Data Shape and Type:\", train_data.shape, train_data.dtype)\n",
    "print(\"Validation Data Shape and Type:\", val_data.shape, val_data.dtype)\n",
    "print(\"Testing Data Shape and Type:\", test_data.shape, test_data.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example show how src and tgt work\n",
    "The tgt of a given src index i should be i+1 in the target tensor. There should be an offset of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 15\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([4]) the target is: 5\n",
      "when input is tensor([4, 5]) the target is: 6\n",
      "when input is tensor([4, 5, 6]) the target is: 7\n",
      "when input is tensor([4, 5, 6, 7]) the target is: 4\n",
      "when input is tensor([4, 5, 6, 7, 4]) the target is: 8\n",
      "when input is tensor([4, 5, 6, 7, 4, 8]) the target is: 9\n",
      "when input is tensor([4, 5, 6, 7, 4, 8, 9]) the target is: 5\n",
      "when input is tensor([4, 5, 6, 7, 4, 8, 9, 5]) the target is: 10\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10]) the target is: 0\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0]) the target is: 6\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6]) the target is: 11\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6, 11]) the target is: 12\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6, 11, 12]) the target is: 13\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6, 11, 12, 13]) the target is: 14\n",
      "when input is tensor([ 4,  5,  6,  7,  4,  8,  9,  5, 10,  0,  6, 11, 12, 13, 14]) the target is: 15\n"
     ]
    }
   ],
   "source": [
    "src = train_data[:block_size]\n",
    "tgt = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = src[:t+1]\n",
    "    target = tgt[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[   48,   131,  1696,    45,     0,    14,  1650, 12931],\n",
      "        [   16,    18, 21257, 10220,    17,    18,  5553,  4046],\n",
      "        [ 2887, 19396,  8651,    14,    38,   207,  5268,    23],\n",
      "        [  119,  8908, 23126,    14,   310,  2597, 23564,    23]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[  131,  1696,    45,     0,    14,  1650, 12931,    14],\n",
      "        [   18, 21257, 10220,    17,    18,  5553,  4046, 20663],\n",
      "        [19396,  8651,    14,    38,   207,  5268,    23,  3724],\n",
      "        [ 8908, 23126,    14,   310,  2597, 23564,    23,   273]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many independant sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions.\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets\")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([48]) the target is: 131\n",
      "when input is tensor([ 48, 131]) the target is: 1696\n",
      "when input is tensor([  48,  131, 1696]) the target is: 45\n",
      "when input is tensor([  48,  131, 1696,   45]) the target is: 0\n",
      "when input is tensor([  48,  131, 1696,   45,    0]) the target is: 14\n",
      "when input is tensor([  48,  131, 1696,   45,    0,   14]) the target is: 1650\n",
      "when input is tensor([  48,  131, 1696,   45,    0,   14, 1650]) the target is: 12931\n",
      "when input is tensor([   48,   131,  1696,    45,     0,    14,  1650, 12931]) the target is: 14\n",
      "when input is tensor([16]) the target is: 18\n",
      "when input is tensor([16, 18]) the target is: 21257\n",
      "when input is tensor([   16,    18, 21257]) the target is: 10220\n",
      "when input is tensor([   16,    18, 21257, 10220]) the target is: 17\n",
      "when input is tensor([   16,    18, 21257, 10220,    17]) the target is: 18\n",
      "when input is tensor([   16,    18, 21257, 10220,    17,    18]) the target is: 5553\n",
      "when input is tensor([   16,    18, 21257, 10220,    17,    18,  5553]) the target is: 4046\n",
      "when input is tensor([   16,    18, 21257, 10220,    17,    18,  5553,  4046]) the target is: 20663\n",
      "when input is tensor([2887]) the target is: 19396\n",
      "when input is tensor([ 2887, 19396]) the target is: 8651\n",
      "when input is tensor([ 2887, 19396,  8651]) the target is: 14\n",
      "when input is tensor([ 2887, 19396,  8651,    14]) the target is: 38\n",
      "when input is tensor([ 2887, 19396,  8651,    14,    38]) the target is: 207\n",
      "when input is tensor([ 2887, 19396,  8651,    14,    38,   207]) the target is: 5268\n",
      "when input is tensor([ 2887, 19396,  8651,    14,    38,   207,  5268]) the target is: 23\n",
      "when input is tensor([ 2887, 19396,  8651,    14,    38,   207,  5268,    23]) the target is: 3724\n",
      "when input is tensor([119]) the target is: 8908\n",
      "when input is tensor([ 119, 8908]) the target is: 23126\n",
      "when input is tensor([  119,  8908, 23126]) the target is: 14\n",
      "when input is tensor([  119,  8908, 23126,    14]) the target is: 310\n",
      "when input is tensor([  119,  8908, 23126,    14,   310]) the target is: 2597\n",
      "when input is tensor([  119,  8908, 23126,    14,   310,  2597]) the target is: 23564\n",
      "when input is tensor([  119,  8908, 23126,    14,   310,  2597, 23564]) the target is: 23\n",
      "when input is tensor([  119,  8908, 23126,    14,   310,  2597, 23564,    23]) the target is: 273\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Data Into a Mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "For understanding purposes and to get something working quickly, we're going to use a basic Bigram language model to work with our data. We'll build something bigger later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size) -> None:\n",
    "    super().__init__()\n",
    "    # each token directly reads off the logits for the next token from a lookup table\n",
    "    num_embedding_dim = 32 \n",
    "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=num_embedding_dim)\n",
    "    self.lm_head = nn.Linear(num_embedding_dim, vocab_size)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, num_embedding_dim)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    token_embeddings = self.token_embedding_table(idx) # (batch by time by channel) tensor\n",
    "    pos_embeddings = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "    x = token_embeddings + pos_embeddings\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      # we need to reshape our logits because the loss function expects (B by C by T)\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss # scores for next char sequence \n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of inidices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      logits, loss = self(idx)          # get the predictions \n",
    "      logits = logits[:, -1, :]         # focus only on last time step\n",
    "      probs = F.softmax(logits, dim=-1) # apply softmax to get probabilities\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # sample from the distrabution\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28785])\n",
      "tensor(10.8159, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "model = BigramLanguageModel(vocab_size) #.to(device)\n",
    "\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape) # (batch_size*block_size, number_of_tokens)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Text\n",
    "We will use the generate function to make 100 tokens starting with 0th token. Note that this will make garbage due to the model being entirley random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'trumpet', 'asphalt', 'exploits', 'org', 'litigators', 'condemned', 'birthplace', 'periodically', 'idyllwild', 'harmonix', 'nominated', 'wet', 'monstrous', 'hugs', 'estate', 'schlesinger', 'conceal', 'appears', 'commandment', 'rolling', 'sunshine', 'kubica', 'a320', 'dodge', 'shirts', 'mechanism', 'sloping', 'peat', 'coaster', 'collections', 'replayed', 'freely', 'determines', 'brenton', 'proves', 'conformist', 'seniors', 'bloody', 'neighborhood', 'sexually', 'command', 'coups', 'twinned', 'relatable', 'desktop', 'humphrey', 'wallabies', 'speculator', 'exit', 'declaration', 'chaotic', 'moravec', 'hunwick', 'est', 'commitment', 'smashed', 'weapons', 'essentially', 'dynamite', 'schopenhauer', 'beatty', '1518', 'winchester', 'themed', 'make', 'halliwell', 'void', 'virampattinam', 'obo', 'right', 'kesselring', 'cooperate', 'vipers', 'garter', 'dumps', 'smackdown', 'inaction', 'shimitsu', 'robbery', 'pleading', 'predications', 'criticisms', 'developer', 'hovers', 'italo', 'midnight', 'oklahoma', '300th', 'drafts', 'fantasies', 'cardell', 'vitro', 'hold', 'vulgaris', 'remedial', 'dale', 'fires', 'machiavellian', 'currency', 'consequently']\n"
     ]
    }
   ],
   "source": [
    "# create 1 by 1 tensor which holds a zero \n",
    "idx = torch.zeros((1,1), dtype=torch.long) \n",
    "\n",
    "# kick off the generation with a zero\n",
    "gen_idx = model.generate(idx, max_new_tokens=100)[0].tolist() # create list of words\n",
    "decoded_gen_idx = [vocab.get_itos()[item] for item in gen_idx]\n",
    "print(decoded_gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1):\n",
    "    # sample a batch of training data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # eval the loss \n",
    "    logits, loss = model(xb, yb)\n",
    "    # opt.zero_grad(set_to_none=True)\n",
    "    # loss.backward()\n",
    "    # opt.step()\n",
    "\n",
    "    # print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad() # tells pytorch nothing here will need to be called .backward() on\n",
    "# def estimate_loss():\n",
    "#   out = {}\n",
    "#   model.eval()\n",
    "#   for split in [\"train\", \"val\"]:\n",
    "#     losses = torch.zeros(eval_iters)\n",
    "#     for k in range(eval_iters):\n",
    "#       X, y = get_batch(split)\n",
    "#       logits, lofss = model(X, y)\n",
    "#       losses[k] = loss.item()\n",
    "#     out[split] = losses.mean()\n",
    "#   model.train()\n",
    "#   return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematical Trick in Self-Attention\n",
    "There should be some \"communication\" between the ith token and all previous tokens (the attention component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4, 8, 2 # batch, time component, channels (some info at each point in sequence)\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is printed out X and the averaged x_bag_of_words. \n",
    "- x[0] is the 0th batch element\n",
    "\n",
    "The first row of x and the bag of words will be the same. for each bag of words row after, the values will be the average of each above row in x."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "tensor([[-0.6284,  1.9393],\n",
      "        [ 0.9123, -0.6665],\n",
      "        [-1.2152, -0.8152],\n",
      "        [-0.5096,  0.6938],\n",
      "        [ 0.7681, -1.3212],\n",
      "        [ 0.0314, -1.0469],\n",
      "        [-1.1389, -0.2334],\n",
      "        [ 0.0226,  0.8392]])\n",
      "X bag of words\n",
      "tensor([[-0.6284,  1.9393],\n",
      "        [ 0.1419,  0.6364],\n",
      "        [-0.3105,  0.1526],\n",
      "        [-0.3602,  0.2879],\n",
      "        [-0.1346, -0.0339],\n",
      "        [-0.1069, -0.2028],\n",
      "        [-0.2543, -0.2072],\n",
      "        [-0.2197, -0.0764]])\n"
     ]
    }
   ],
   "source": [
    "x_bag_of_words = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "  for time in range(T):\n",
    "    x_prev = x[b, :time+1] # (t, C) \n",
    "    x_bag_of_words[b,time] = torch.mean(x_prev, 0)\n",
    "\n",
    "print(\"X:\")\n",
    "print(x[0])\n",
    "print(\"X bag of words\")\n",
    "print(x_bag_of_words[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2: We can do this more efficiently with matrix multiplication. We are doing weighted sums to get a triangular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are Close: True\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "\n",
    "x_bag_of_words_2 = weights @ x # (B, T, T) matmul by (B, T, C) --> (B, T, C)\n",
    "print(\"Are Close:\", torch.allclose(x_bag_of_words, x_bag_of_words_2))\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3: Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked fill:\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Are Close: True\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\")) # future cant communicate with the past\n",
    "print(\"masked fill:\")\n",
    "print(weights)\n",
    "\n",
    "weights = F.softmax(weights, dim=-1) # normalize\n",
    "x_bag_of_words_3 = weights @ x  # sum\n",
    "print(\"Are Close:\", torch.allclose(x_bag_of_words, x_bag_of_words_3))\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention - Most important Part\n",
    "\n",
    "NOTES\n",
    "- It is a communication mechanism. Can be seen as nodes in a directed graph. Edges point between nodes. Every node has a vector of information, and it gets to aggregate information from all nodes pointed to it.\n",
    "\n",
    "- In our graph each token is pointed to by itself and each previous token.\n",
    "\n",
    "- There is no notion of space. This is why we need our positional encodings to inform the graph of order. \n",
    "\n",
    "- Each example accross batch dimensions are compeletly independant and never talk to each other.\n",
    "\n",
    "- \"self-attention\" is called as such because all of the information which defines the attention *weights* comes from *x*\n",
    "\n",
    "- \"scaled attention\" when query Q and key K are unit varience, weights will be unit varience too and softmax will stay diffuse and not saturate too much.\n",
    "\n",
    "Every single node/token will emit two vectors \n",
    "- query (what am I looking for)\n",
    "- key (what do I contain)\n",
    "\n",
    "query is dot producted with keys to make weights\n",
    "\n",
    "We are going to make one \"head\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7179, 0.2821, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1932, 0.3257, 0.4811, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4252, 0.1471, 0.2120, 0.2157, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3869, 0.1577, 0.1527, 0.1365, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2094, 0.1212, 0.1904, 0.1660, 0.1020, 0.2110, 0.0000, 0.0000],\n",
      "        [0.1112, 0.1016, 0.1692, 0.2236, 0.0995, 0.1818, 0.1131, 0.0000],\n",
      "        [0.0923, 0.1413, 0.1114, 0.1750, 0.1668, 0.0749, 0.1413, 0.0968]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# lets see a single self-attention head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16) <- 16 is from head_size\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # value is used for elements that we aggregate rather than x\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, 16) matmul (B, 16, T) ---> (B, T, T)\n",
    "weights *= head_size**-0.5 # \"scaled attention\" - (1/sqrt(head_size))\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\")) # future cant communicate with the past\n",
    "\n",
    "weights = F.softmax(weights, dim=-1) # normalize\n",
    "x_bag_of_words_4 = weights @ v  # sum\n",
    "\n",
    "print(weights[0])\n",
    "\n",
    "print(x_bag_of_words_4.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "num_embedding_dim = 384\n",
    "num_attn_heads = 6\n",
    "num_layers = 6 \n",
    "dropout = 0.2 # random shuts off some subset of nuerons, allows prevention of overfitting\n",
    "\n",
    "if device == \"cpu\":\n",
    "  # if not on GPU, drop the parameters down\n",
    "  num_embedding_dim = 8\n",
    "  num_attn_heads = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Head Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  \"\"\"One head of self-attention\"\"\"\n",
    "  def __init__(self, head_size) -> None:\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(C, head_size, bias=False)\n",
    "    self.query = nn.Linear(C, head_size, bias=False)\n",
    "    self.value = nn.Linear(C, head_size, bias=False)\n",
    "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    k = key(x) # (B, T, 16) <- 16 is from head_size\n",
    "    q = query(x) # (B, T, 16)\n",
    "    v = value(x) # value is used for elements that we aggregate rather than x\n",
    "\n",
    "    weights = q @ k.transpose(-2, -1) # (B, T, 16) matmul (B, 16, T) ---> (B, T, T)\n",
    "    weights *= C**-0.5 # \"scaled attention\" - (1/sqrt(head_size))\n",
    "    weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # future cant communicate with the past\n",
    "    weights = F.softmax(weights, dim=-1) # normalize\n",
    "    weights = self.dropout(weights)\n",
    "    out = weights @ v  # sum\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "multi-head attention is multiple heads of attention running in parallel and then the results are concat'ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, num_heads, head_size) -> None:\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n",
    "    self.proj = nn.Linear(num_embedding_dim, num_embedding_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.dropout(self.proj(out))\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward layers of transformer\n",
    "The tokens looked at each other but didn't have time to \"think\" about what the others meant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "  def __init__(self, num_embedding_dim) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    # there is a ration of 4:1 for input/output for d_model and the inner-layer dimensionality\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(num_embedding_dim, 4 * num_embedding_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4 * num_embedding_dim, num_embedding_dim), # projection layer\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  \"\"\"Transformer block, communication followed by computation\"\"\"\n",
    "  def __init__(self, num_embedding_dim, num_heads) -> None:\n",
    "    super().__init__()\n",
    "    head_size = num_embedding_dim // num_heads\n",
    "    self.self_attn = MultiHeadAttention(\n",
    "      num_heads=num_heads,\n",
    "      head_size=head_size\n",
    "    )\n",
    "    self.ffwd = FeedForward(num_embedding_dim)\n",
    "    self.layer_norm_1 = nn.LayerNorm(num_embedding_dim)\n",
    "    self.layer_norm_2 = nn.LayerNorm(num_embedding_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.self_attn(self.layer_norm_1(x))\n",
    "    x = x + self.ffwd(self.layer_norm_2(x))\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layernorm\n",
    "normalizing columns of input, I'm using the provided pytorch implementation rather than writing my own"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "vocab_size = vocab_size\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    # each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=num_embedding_dim)\n",
    "    self.position_embedding_table = nn.Embedding(block_size, num_embedding_dim)\n",
    "    \n",
    "    self.blocks = nn.Sequential(\n",
    "      Block(num_embedding_dim, num_heads=num_attn_heads),\n",
    "      Block(num_embedding_dim, num_heads=num_attn_heads),\n",
    "      Block(num_embedding_dim, num_heads=num_attn_heads),\n",
    "      nn.LayerNorm(num_embedding_dim)\n",
    "    )\n",
    "    self.ffwd = FeedForward(num_embedding_dim)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    token_embeddings = self.token_embedding_table(idx) # (batch by time by channel) tensor\n",
    "    pos_embeddings = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "    x = token_embeddings + pos_embeddings\n",
    "    x = self.blocks(x) # (B, T, C)\n",
    "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    if targets == None:\n",
    "      loss = None\n",
    "    else:\n",
    "      # we need to reshape our logits because the loss function expects (B by C by T)\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss # scores for next char sequence \n",
    "  \n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of inidices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      idx_cond = idx[:, -block_size]    # crop idx to the last block_size tokens\n",
    "      logits, loss = self(idx_cond)     # get the predictions \n",
    "      logits = logits[:, -1, :]         # focus only on last time step\n",
    "      probs = F.softmax(logits, dim=-1) # apply softmax to get probabilities\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # sample from the distrabution\n",
    "      idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Review of the code\n",
    "We have written a decoder only transformer, it is only capable of making random output.\n",
    "\n",
    "We will have to implement our own encoder block should we want to take in tokens on the fly as input. This encoder wouldnt have the same -nf mask as the decoder but would have attention to everything. **should be at around 1:40:00 in the video**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining vs Fine Tuning\n",
    "\n",
    "#### Pretraining\n",
    "In the Chat-GPT paper it contains a table with the hyperparameters they used. This setp is the main bulk of the training where you load in massive sets of parameters. \n",
    "\n",
    "#### Fine tuning\n",
    "Rather than spitting out garbage in attempt to finish sequences you want to optimize it for a certain task. Chat-GPT's website give a general 3 step outline for how this should be done. \n",
    "1. Collection demonstration data and train a supervised policy.\n",
    "2. Collect comparison data and train a reward model\n",
    "3. Optimize a policy against the reward model using the PPO reinforcement learning algo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
