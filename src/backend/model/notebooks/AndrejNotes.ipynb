{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Notes\n",
    "This notebook will be based off of [this video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which goes into depth on how to build the exact base model I need for this project. I will be copy pasting a lot of his work and annotating it to help myself understand the process of making the Transformer work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "\n",
    "# pytorch functionality\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# data\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "For this example I am going to be using the IMDB dataset but the work/model should be generalizable to all text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = WikiText2()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "For now I'm going to keep my tokenizer very simple. You can use a multitude of techniques for tokenizing your corpus. Here is a [library](https://github.com/openai/tiktoken) worth looking into at some point.\n",
    "\n",
    "We are going to be getting very long sequences but small token spaces. This can be changed with this library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Helper Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(in_data, tokenizer):\n",
    "  counter = Counter()\n",
    "  for string in in_data:\n",
    "    counter.update(tokenizer(string))\n",
    "\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "def data_process(in_data, tokenizer, vocab: Vocab):\n",
    "  raw_iter = iter(in_data)\n",
    "  data = []\n",
    "  for raw in raw_iter:\n",
    "    tensor = torch.tensor([vocab[token] for token in tokenizer(raw)], dtype=torch.long)\n",
    "    data.append(tensor)\n",
    "    \n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Tokenizer\n",
    "\n",
    "I am going to be doing something slightly different to the video. I'm choosing to use the provided torch tokenizer for words rather than doing it char by char. Torch's tools support this kind of work more but it will require some slight adjustments to the work done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vocab Size: 28785\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(train_iter, tokenizer)\n",
    "\n",
    "print(\"Train Vocab Size:\", len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Example for how encoding and decoding works with Vocab Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'There' after encoding: 248\n",
      "'There' after decoding there\n"
     ]
    }
   ],
   "source": [
    "encoded_word = vocab.get_stoi()[\"there\"]\n",
    "decoded_word = vocab.get_itos()[encoded_word]\n",
    "\n",
    "print(\"'There' after encoding:\", encoded_word)\n",
    "print(\"'There' after decoding\", decoded_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Tensor Format\n",
    "Using above data_process function, build a torch tensor representation based on the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape and Type: torch.Size([2049990]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_data = data_process(train_iter, tokenizer, vocab)\n",
    "val_data = data_process(val_iter, tokenizer, vocab)\n",
    "test_data = data_process(test_iter, tokenizer, vocab)\n",
    "\n",
    "print(\"Training Data Shape and Type:\", train_data.shape, train_data.dtype)\n",
    "print(\"Validation Data Shape and Type:\", val_data.shape, val_data.dtype)\n",
    "print(\"Testing Data Shape and Type:\", test_data.shape, test_data.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
