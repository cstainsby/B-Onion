{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Am I the Asshole EDA\n",
    "This file will be dedicated to explore how to craft responses to posts in the subreddit r/AmITheAsshole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "\n",
    "# load enviornment variables for praw to work later\n",
    "load_dotenv(dotenv_path=Path(\"../settings.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to check if table exists\n",
    "def gcp_table_exists(client: bigquery.Client, table_id: str):\n",
    "    try:\n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Using Google Bigquery, create datasets for the data that is about to be loaded. The dataset will be called **AITA_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_NAME = \"bonion\"\n",
    "DATASET_NAME = \"AITA_dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all dataset ID's\n",
    "To prevent overwriting pre-existing datasets, get all dataset id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datasets \n",
    "datasets = list(client.list_datasets())  # Make an API request.\n",
    "dataset_ids = [dataset.dataset_id for dataset in datasets]\n",
    "project = client.project\n",
    "\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "if DATASET_NAME not in dataset_ids:\n",
    "    dataset = bigquery.Dataset(\"{}.{}\".format(PROJ_NAME, DATASET_NAME))\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    # send dataset to API for completion\n",
    "    initial_post_dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Table\n",
    "The initial post for each of the comments will need to be stored to reference its content which dictated the reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table id definitions\n",
    "post_table_id = \"{}.{}.post_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "post_reply_table_id = \"{}.{}.post_reply_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "post_reply_top_children_table_id = \"{}.{}.post_reply_top_children\".format(PROJ_NAME, DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all Table ID's\n",
    "To prevent overwriting pre-existing tables, get all table id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list(client.list_tables(DATASET_NAME))  # Make an API request.\n",
    "table_ids = [table.table_id for table in tables]\n",
    "\n",
    "print(\"Tables contained in '{}':\".format(DATASET_NAME))\n",
    "for table in tables:\n",
    "    print(\"{}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_table_schema = [\n",
    "    bigquery.SchemaField(\"reddit_post_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_title\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_self_text\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_table_id): \n",
    "    post_table = bigquery.Table(post_table_id, schema=post_table_schema)\n",
    "    post_table.description = \"\"\"\n",
    "        A table which holds popular posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Reply Table\n",
    "This table holds all of the responses for each of the posts in the post table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_reply_table_schema = [\n",
    "    bigquery.SchemaField(\"reddit_post_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_contents\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_reply_table_id): \n",
    "    post_reply_table = bigquery.Table(post_reply_table_id, schema=post_reply_table_schema)\n",
    "    post_reply_table.description = \"\"\"\n",
    "        A table which holds the most popular replys to saved posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Reply Top Children\n",
    "This table will hold replys to the initial post reply. This will be used for also dictating the performance of a reply. Because there can be infinite replies to any given post or reply, each post reply id will be limited to some number of replies. These replies will likley be based off of the same metrics as the original post for \"quality\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_reply_top_children_table_schema = [\n",
    "    bigquery.SchemaField(\"parent_comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_contents\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]   \n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_reply_top_children_table_id): \n",
    "    post_reply_top_children_table = bigquery.Table(post_reply_top_children_table_id, schema=post_reply_top_children_table_schema)\n",
    "    post_reply_top_children_table.description = \"\"\"\n",
    "        This table will hold replys to the initial post reply. \n",
    "        This will be used for also dictating the performance of a reply. \n",
    "        Because there can be infinite replies to any given post or reply, \n",
    "        each post reply id will be limited to some number of replies. \n",
    "        These replies will likley be based off of the same metrics as \n",
    "        the original post for \"quality\". \n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_top_children_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "Using my PrawInstance Object, I will be collecting the top posts from the subreddit. I will be storing this data in google bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw_instance\n",
    "importlib.reload(praw_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = \"amitheasshole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "praw_inst = praw_instance.PrawInstance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Top 1000 Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10000_posts = praw_instance.get_top_by_subreddit(praw_inst, subreddit_name, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the top 1000 posts into gcp\n",
    "# convert data into json rows \n",
    "post_table_df = praw_instance.post_dict_to_df(top_10000_posts)\n",
    "\n",
    "post_table_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_table_df[\"reddit_post_id\"] = post_table_df[\"reddit_post_id\"].astype(str)\n",
    "post_table_df[\"post_title\"] = post_table_df[\"post_title\"].astype(str)\n",
    "post_table_df[\"post_self_text\"] = post_table_df[\"post_self_text\"].astype(str)\n",
    "post_table_df[\"upvotes\"] = post_table_df[\"upvotes\"].astype(int)\n",
    "post_table_df[\"num_responses\"] = post_table_df[\"num_responses\"].astype(int)\n",
    "post_table_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_gbq.to_gbq(post_table_df, post_table_id, project_id=PROJ_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top Comments and Replies for the top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top post ids\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT reddit_post_id\n",
    "    FROM {}\n",
    "\"\"\".format(post_table_id)\n",
    "\n",
    "post_table_ids_df = pd.read_gbq(query, project_id=PROJ_NAME)\n",
    "\n",
    "post_table_ids_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_and_reply_dict_list = []\n",
    "\n",
    "for index, row in post_table_ids_df.iterrows():\n",
    "    comment_and_reply_dict = praw_instance.get_top_comments_and_top_replies_by_post_id(\n",
    "        praw_inst,\n",
    "        row[\"reddit_post_id\"],\n",
    "        comment_limit=10,\n",
    "        reply_limit=5\n",
    "    )\n",
    "\n",
    "    comment_and_reply_dict_list.append(comment_and_reply_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
