{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Am I the Asshole EDA\n",
    "This file will be dedicated to explore how to craft responses to posts in the subreddit r/AmITheAsshole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(\"..\")\n",
    "\n",
    "# load enviornment variables for praw to work later\n",
    "load_dotenv(dotenv_path=Path(\"../settings.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to check if table exists\n",
    "def gcp_table_exists(client: bigquery.Client, table_id: str):\n",
    "    try:\n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Using Google Bigquery, create datasets for the data that is about to be loaded. The dataset will be called **AITA_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_NAME = \"bonion\"\n",
    "DATASET_NAME = \"AITA_dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all dataset ID's\n",
    "To prevent overwriting pre-existing datasets, get all dataset id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in project bonion:\n",
      "\tAITA_dataset\n"
     ]
    }
   ],
   "source": [
    "# Get all datasets \n",
    "datasets = list(client.list_datasets())  # Make an API request.\n",
    "dataset_ids = [dataset.dataset_id for dataset in datasets]\n",
    "project = client.project\n",
    "\n",
    "if datasets:\n",
    "    print(\"Datasets in project {}:\".format(project))\n",
    "    for dataset in datasets:\n",
    "        print(\"\\t{}\".format(dataset.dataset_id))\n",
    "else:\n",
    "    print(\"{} project does not contain any datasets.\".format(project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "if DATASET_NAME not in dataset_ids:\n",
    "    dataset = bigquery.Dataset(\"{}.{}\".format(PROJ_NAME, DATASET_NAME))\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    # send dataset to API for completion\n",
    "    initial_post_dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Table\n",
    "The initial post for each of the comments will need to be stored to reference its content which dictated the reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table id definitions\n",
    "post_table_id = \"{}.{}.post_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "post_reply_table_id = \"{}.{}.post_reply_table\".format(PROJ_NAME, DATASET_NAME)\n",
    "post_reply_top_children_table_id = \"{}.{}.post_reply_top_children\".format(PROJ_NAME, DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all Table ID's\n",
    "To prevent overwriting pre-existing tables, get all table id's to check before any creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables contained in 'AITA_dataset':\n",
      "bonion.AITA_dataset.post_reply_table\n",
      "bonion.AITA_dataset.post_reply_top_children\n",
      "bonion.AITA_dataset.post_table\n"
     ]
    }
   ],
   "source": [
    "tables = list(client.list_tables(DATASET_NAME))  # Make an API request.\n",
    "table_ids = [table.table_id for table in tables]\n",
    "\n",
    "print(\"Tables contained in '{}':\".format(DATASET_NAME))\n",
    "for table in tables:\n",
    "    print(\"{}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_table_schema = [\n",
    "    bigquery.SchemaField(\"reddit_post_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_title\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"post_self_text\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_table_id): \n",
    "    post_table = bigquery.Table(post_table_id, schema=post_table_schema)\n",
    "    post_table.description = \"\"\"\n",
    "        A table which holds popular posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Reply Table\n",
    "This table holds all of the responses for each of the posts in the post table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_reply_table_schema = [\n",
    "    bigquery.SchemaField(\"reddit_post_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_contents\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_reply_table_id): \n",
    "    post_reply_table = bigquery.Table(post_reply_table_id, schema=post_reply_table_schema)\n",
    "    post_reply_table.description = \"\"\"\n",
    "        A table which holds the most popular replys to saved posts from the subreddit r/AITA\n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Reply Top Children\n",
    "This table will hold replys to the initial post reply. This will be used for also dictating the performance of a reply. Because there can be infinite replies to any given post or reply, each post reply id will be limited to some number of replies. These replies will likley be based off of the same metrics as the original post for \"quality\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_reply_top_children_table_schema = [\n",
    "    bigquery.SchemaField(\"parent_comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"comment_contents\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"upvotes\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"num_responses\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "]   \n",
    "\n",
    "if not gcp_table_exists(client, table_id=post_reply_top_children_table_id): \n",
    "    post_reply_top_children_table = bigquery.Table(post_reply_top_children_table_id, schema=post_reply_top_children_table_schema)\n",
    "    post_reply_top_children_table.description = \"\"\"\n",
    "        This table will hold replys to the initial post reply. \n",
    "        This will be used for also dictating the performance of a reply. \n",
    "        Because there can be infinite replies to any given post or reply, \n",
    "        each post reply id will be limited to some number of replies. \n",
    "        These replies will likley be based off of the same metrics as \n",
    "        the original post for \"quality\". \n",
    "    \"\"\"\n",
    "\n",
    "    table = client.create_table(post_reply_top_children_table)\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "Using my PrawInstance Object, I will be collecting the top posts from the subreddit. I will be storing this data in google bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'praw_instance' from '/home/cstainsby/class/dataProj/bonion/src/notebooks/../praw_instance.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw_instance\n",
    "importlib.reload(praw_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_name = \"amitheasshole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "praw_inst = praw_instance.PrawInstance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10000_posts = praw_instance.get_top_by_subreddit(praw_inst, subreddit_name, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full data shape (998, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_self_text</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocx94s</td>\n",
       "      <td>AITA for telling my wife the lock on my daught...</td>\n",
       "      <td>My brother in-law (Sammy) lost his home shortl...</td>\n",
       "      <td>81021</td>\n",
       "      <td>5279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6xoro</td>\n",
       "      <td>META: This sub is moving towards a value syste...</td>\n",
       "      <td>I’ve enjoyed reading and posting on this sub f...</td>\n",
       "      <td>80910</td>\n",
       "      <td>6190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azvko1</td>\n",
       "      <td>UPDATE, AITA for despising my mentally handica...</td>\n",
       "      <td>I'm back like I said I would be,. My [original...</td>\n",
       "      <td>72780</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gr8bp3</td>\n",
       "      <td>AITA For suing my girlfriend after she had my ...</td>\n",
       "      <td>I'll try to keep this short. I had a [1967 Imp...</td>\n",
       "      <td>70814</td>\n",
       "      <td>2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x2k5kv</td>\n",
       "      <td>AITA for bringing my SIL’s wallet to the resta...</td>\n",
       "      <td>Edit: update on profile\\n\\nMy (f28) SIL “Amy” ...</td>\n",
       "      <td>69793</td>\n",
       "      <td>3822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cjetsa</td>\n",
       "      <td>UPDATE: AITA for wanting to go to the funeral ...</td>\n",
       "      <td>I want to sincerely thank everyone who comment...</td>\n",
       "      <td>67572</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e5k3z2</td>\n",
       "      <td>AITA for pretending to get fired when customer...</td>\n",
       "      <td>I am a high schooler with a weekend job at a c...</td>\n",
       "      <td>63528</td>\n",
       "      <td>3621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zvmflw</td>\n",
       "      <td>AITA for bringing up my brother's \"premature\" ...</td>\n",
       "      <td>I am a nurse practitioner and I am the primary...</td>\n",
       "      <td>59171</td>\n",
       "      <td>3370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flan73</td>\n",
       "      <td>UPDATE: WIBTA if I took over planning my own f...</td>\n",
       "      <td>Hello, everyone. First of all, thank you all f...</td>\n",
       "      <td>58489</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dhfeg9</td>\n",
       "      <td>AITA for making a dad joke?</td>\n",
       "      <td>Note. My step-daughter, Madeline, was about a ...</td>\n",
       "      <td>56954</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reddit_post_id                                         post_title  \\\n",
       "0         ocx94s  AITA for telling my wife the lock on my daught...   \n",
       "1         d6xoro  META: This sub is moving towards a value syste...   \n",
       "2         azvko1  UPDATE, AITA for despising my mentally handica...   \n",
       "3         gr8bp3  AITA For suing my girlfriend after she had my ...   \n",
       "4         x2k5kv  AITA for bringing my SIL’s wallet to the resta...   \n",
       "5         cjetsa  UPDATE: AITA for wanting to go to the funeral ...   \n",
       "6         e5k3z2  AITA for pretending to get fired when customer...   \n",
       "7         zvmflw  AITA for bringing up my brother's \"premature\" ...   \n",
       "8         flan73  UPDATE: WIBTA if I took over planning my own f...   \n",
       "9         dhfeg9                        AITA for making a dad joke?   \n",
       "\n",
       "                                      post_self_text upvotes num_responses  \n",
       "0  My brother in-law (Sammy) lost his home shortl...   81021          5279  \n",
       "1  I’ve enjoyed reading and posting on this sub f...   80910          6190  \n",
       "2  I'm back like I said I would be,. My [original...   72780          1985  \n",
       "3  I'll try to keep this short. I had a [1967 Imp...   70814          2757  \n",
       "4  Edit: update on profile\\n\\nMy (f28) SIL “Amy” ...   69793          3822  \n",
       "5  I want to sincerely thank everyone who comment...   67572             2  \n",
       "6  I am a high schooler with a weekend job at a c...   63528          3621  \n",
       "7  I am a nurse practitioner and I am the primary...   59171          3370  \n",
       "8  Hello, everyone. First of all, thank you all f...   58489             2  \n",
       "9  Note. My step-daughter, Madeline, was about a ...   56954          1995  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the top 1000 posts into gcp\n",
    "# convert data into json rows \n",
    "post_table_df = praw_instance.post_dict_to_df(top_10000_posts)\n",
    "\n",
    "post_table_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not determine schema for table 'bonion.AITA_dataset.post_table'. Call client.get_table() or pass in a list of schema fields to the selected_fields argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# job_config = bigquery.LoadJobConfig(schema=post_table_schema)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m errors \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49minsert_rows_from_dataframe(\n\u001b[1;32m      3\u001b[0m     post_table_id,\n\u001b[1;32m      4\u001b[0m     post_table_df\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/bigquery/client.py:3549\u001b[0m, in \u001b[0;36mClient.insert_rows_from_dataframe\u001b[0;34m(self, table, dataframe, selected_fields, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   3547\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(chunk_count):\n\u001b[1;32m   3548\u001b[0m     rows_chunk \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mislice(rows_iter, chunk_size)\n\u001b[0;32m-> 3549\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minsert_rows(table, rows_chunk, selected_fields, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3550\u001b[0m     insert_results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m   3552\u001b[0m \u001b[39mreturn\u001b[39;00m insert_results\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/bigquery/client.py:3483\u001b[0m, in \u001b[0;36mClient.insert_rows\u001b[0;34m(self, table, rows, selected_fields, **kwargs)\u001b[0m\n\u001b[1;32m   3480\u001b[0m     schema \u001b[39m=\u001b[39m selected_fields\n\u001b[1;32m   3482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(schema) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 3483\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3484\u001b[0m         (\n\u001b[1;32m   3485\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCould not determine schema for table \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Call client.get_table() \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3486\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor pass in a list of schema fields to the selected_fields argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3487\u001b[0m         )\u001b[39m.\u001b[39mformat(table)\n\u001b[1;32m   3488\u001b[0m     )\n\u001b[1;32m   3490\u001b[0m json_rows \u001b[39m=\u001b[39m [_record_field_to_json(schema, row) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m rows]\n\u001b[1;32m   3492\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minsert_rows_json(table, json_rows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not determine schema for table 'bonion.AITA_dataset.post_table'. Call client.get_table() or pass in a list of schema fields to the selected_fields argument."
     ]
    }
   ],
   "source": [
    "# job_config = bigquery.LoadJobConfig(schema=post_table_schema)\n",
    "errors = client.insert_rows_from_dataframe(\n",
    "    post_table_id,\n",
    "    post_table_df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
